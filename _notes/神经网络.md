# 基本组成元素

神经元
![](/assets/images/神经网络-1.png)
单层网络
![](/assets/images/神经网络-2.png)
多层网络
![](/assets/images/神经网络-3.png)

> 当只使用线性函数，多层网络会坍塌为单层网络，故使用非线性的激活函数，也可提高网络对复杂样本的拟合性。

输出层：单输出。
![](/assets/images/神经网络-4.png)
输出层：多分类。
![](/assets/images/神经网络-5.png)

# 训练神经网络

设置训练目标，均方差。
![](/assets/images/神经网络-6.png)
设置训练目标，交叉熵。
![](/assets/images/神经网络-7.png)

> 一个好的损失函数要能捕捉到不同模型间预测效果的差异，比如都是预测错误，但能体现其预测错误的概率差异。

最小化损失函数：梯度下降。
- 求梯度：偏导、链式法则、反向传播。

![](/assets/images/神经网络-8.png)
![](/assets/images/神经网络-9.png)

# 词向量 word2vec

两种模型架构：CBOW、skip-gram。捕捉词语之间的关系。
![](/assets/images/神经网络-10.png)

# 神经网络模型

**RNN（循环神经网络）**、**GRU（门控循环单元）** 和 **LSTM（长短期记忆网络）** 是深度学习中用于处理序列数据的三种经典神经网络模型。

## 循环神经网络

RNN Language Model
- RNN 通过循环结构处理序列数据，每个时间步的输入不仅包括当前时刻的数据，还包括上一时刻的隐藏状态。
- 隐藏状态可以看作是对过去信息的记忆。
![](/assets/images/神经网络-11.png)

## 门控循环单元

GRU Gated Recurrent Unit
- GRU 是 RNN 的改进版本，通过引入**门控机制**（更新门和重置门）来解决 RNN 的梯度消失问题。
- GRU 比 LSTM 更简单，计算效率更高。
![](/assets/images/神经网络-12.png)

## 长短期记忆网络

LSTM Long Short-Term Memory
- LSTM 是 RNN 的另一种改进版本，通过引入**三个门控机制**（输入门、遗忘门、输出门）来更好地控制信息的流动。
- LSTM 能够有效地捕捉长距离依赖关系。

![](/assets/images/神经网络-13.png)

传统的 RNN 只能按时间步顺序处理序列数据，而双向 RNN 通过引入两个独立的 RNN 层来同时处理序列：
- **前向 RNN**：按时间步顺序（从开始到结束）处理序列。
- **后向 RNN**：按时间步逆序（从结束到开始）处理序列。

## 卷积神经网络

![](/assets/images/神经网络-14.png)

![](assets/images/神经网络-15.png)

## 模型训练

Base PyTorch。

Pipeline for Deep Learning：Prepare Data > Build Model > Train Model > Evaluate Model > Test Model。








