# 基本组成元素

神经元
![[神经网络-1.png]]
单层网络
![[神经网络-2.png]]
多层网络
![[神经网络-3.png]]

> 当只使用线性函数，多层网络会坍塌为单层网络，故使用非线性的激活函数，也可提高网络对复杂样本的拟合性。

输出层：单输出。
![[神经网络-4.png]]
输出层：多分类。
![[神经网络-5.png]]

# 训练神经网络

设置训练目标，均方差。
![[神经网络-6.png]]
设置训练目标，交叉熵。
![[神经网络-7.png]]

> 一个好的损失函数要能捕捉到不同模型间预测效果的差异，比如都是预测错误，但能体现其预测错误的概率差异。

最小化损失函数：梯度下降。
- 求梯度：偏导、链式法则、反向传播。

![[神经网络-8.png]]
![[神经网络-9.png]]

# 词向量 word2vec

两种模型架构：CBOW、skip-gram。捕捉词语之间的关系。
![[神经网络-10.png]]

# 神经网络模型

**RNN（循环神经网络）**、**GRU（门控循环单元）** 和 **LSTM（长短期记忆网络）** 是深度学习中用于处理序列数据的三种经典神经网络模型。

循环神经网络 RNN Language Model。
- RNN 通过循环结构处理序列数据，每个时间步的输入不仅包括当前时刻的数据，还包括上一时刻的隐藏状态。
- 隐藏状态可以看作是对过去信息的记忆。
![[神经网络-11.png]]

GRU Gated Recurrent Unit 门控循环单元。
- GRU 是 RNN 的改进版本，通过引入**门控机制**（更新门和重置门）来解决 RNN 的梯度消失问题。
- GRU 比 LSTM 更简单，计算效率更高。
![[神经网络-12.png]]

LSTM Long Short-Term Memory 长短期记忆网络。
- LSTM 是 RNN 的另一种改进版本，通过引入**三个门控机制**（输入门、遗忘门、输出门）来更好地控制信息的流动。
- LSTM 能够有效地捕捉长距离依赖关系。










