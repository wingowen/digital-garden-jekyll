---
tags:
  - ML
---
[Machine Learning Notes](https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer)

# 常见神经网络

Convolutional Neural Networks CNN 卷积神经网络，适合于处理具有网格结构的数据，如图像识别、视频分析等领域。

Recurrent Neural Network RNN 递归神经网络，用于处理序列数据的神经网络类型。

Long Short-Term Memory LSTM 长短期记忆网络，RNN 的一种变体，设计用来解决 RNN 在处理长序列时遇到的梯度消失问题，通过引入门控机制来维持长期依赖关系。LSTM 包含三个门（输入门、遗忘门、输出门），分别控制信息的流入、保留和流出。

Gated Recurrent Unit GRU 门控循环单元，RNN 的一种变体，设计用来解决 RNN 在处理长序列时遇到的梯度消失问题，通过引入门控机制来控制信息的流动，但其结构相对简单，因为它只包含两个门：更新门（Update Gate）和重置门（Reset Gate）。

# Transformer
## Attention

Statistical Machine Translation SMT 统计机器翻译，其核心为贝叶斯公式。Neural Machine Translation NMT 的出现统治了这一领域，Attention 是此翻译模型中普遍采用的结构。

# RAG

Retrieval Augmented Generation RAG 检索增强生成。RAG 结合了搜索技术和大语言模型的提示词功能，即向模型提出问题，并以搜索算法找到的信息作为背景上下文，这些查询和检索到的上下文信息都会被整合进发送给大语言模型的提示中。

在基于大语言模型的管道和应用领域，两个最著名的开源库分别是 LangChain 和 LlamaIndex。使用 LlamaIndex 创建矢量索引存储。

- [How To Implement RAG: A Simple Walkthrough](https://dzone.com/articles/how-to-implement-rag-a-simple-walkthrough)