# 原理介绍

注意力机制的核心是计算一个**注意力权重分布**，然后根据这个分布对输入进行加权求和。

一个例子：Seq2Seq with Attention。
![](assets/images/注意力机制-1.png)

# Transformer

![[assets/images/注意力机制-2.png]]
